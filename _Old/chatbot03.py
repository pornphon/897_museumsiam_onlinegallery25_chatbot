import streamlit as st
import os
import pandas as pd
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain_core.documents import Document
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQAWithSourcesChain

# ‡πÇ‡∏´‡∏•‡∏î .env
load_dotenv()
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
INDEX_NAME = os.getenv("PINECONE_INDEX")

llm = ChatOpenAI(model_name="gpt-4o")


# ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Pinecone
pc = Pinecone(api_key=PINECONE_API_KEY)
index = pc.Index(INDEX_NAME)

# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° embeddings
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

stats = index.describe_index_stats()
if stats.total_vector_count == 0:
    df = pd.read_csv("antiquities.csv")

    df = df[df["deleted_at"].isnull()]
    df["thumbnail"] = df["thumbnail"].fillna("")
    df["id"] = df["id"].fillna("").astype(str)

    docs = [
        Document(
            page_content = (
                f"‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÇ‡∏ö‡∏£‡∏≤‡∏ì: {row['name_th']} ({row['name_en']}). "
                f"‡∏ó‡∏≥‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏™‡∏î‡∏∏: {row['material_tags']}. "
                f"‡∏¢‡∏∏‡∏Ñ‡∏™‡∏°‡∏±‡∏¢: {row['period_tags']}. "
                f"‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏ö: {row['place_tags']}."
                f"‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î: {row['artistic_description_th']}."
                f"‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î: {row['artistic_description_en']}."
                f"‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î: {row['historical_background_description_th']}."
                f"‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î: {row['historical_background_description_en']}."
                

            )
        )
        for _, row in df.iterrows()
    ]
    print("üü° Uploading documents to Pinecone...")
    PineconeVectorStore.from_documents(docs, embeddings, index_name=INDEX_NAME)
else:
    print(f"‚úÖ Already have {stats.total_vector_count} vectors in Pinecone.")






# ‡∏™‡∏£‡πâ‡∏≤‡∏á vectorstore
vectorstore = PineconeVectorStore(index=index, embedding=embeddings, text_key="text")

# === Streamlit UI ===
st.set_page_config(page_title="Museum Chatbot", page_icon="üè∫")
st.title("üóø ‡∏ñ‡∏≤‡∏°-‡∏ï‡∏≠‡∏ö‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÇ‡∏ö‡∏£‡∏≤‡∏ì")

question = st.text_input("üì• ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà:")


custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""\
‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÇ‡∏ö‡∏£‡∏≤‡∏ì
‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö 4-5 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ:
{context}

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:
{question}

‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:"""
)

if question:
    with st.spinner("üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å Pinecone..."):
    # mmr
        retriever = vectorstore.as_retriever(
            search_type="mmr",  # üî• Hybrid Search = MMR (Maximal Marginal Relevance)
            search_kwargs={
                "k": 4,             # ‡∏î‡∏∂‡∏á 4 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
                "fetch_k":10,      # ‡∏Ñ‡πâ‡∏ô‡πÄ‡∏¢‡∏≠‡∏∞‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏±‡∏î‡πÉ‡∏´‡πâ‡∏î‡∏µ
            }
        )

        # ‡∏ï‡∏±‡πâ‡∏á RetrievalQA
        qa = RetrievalQA.from_chain_type(
            llm=llm,
            retriever=retriever,
            chain_type="stuff",
            chain_type_kwargs={"prompt": custom_prompt},
            return_source_documents=True,
        )
        result = qa.invoke({"query": question})

        st.subheader("üìù ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏£‡∏∏‡∏õ:")
        st.success(result["result"])

        st.divider()
        st.subheader("üìö ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ö:")
        for i, doc in enumerate(result["source_documents"], start=1):
            st.markdown(f"**{i}.** {doc.page_content}")

            if "thumbnail" in doc.metadata and doc.metadata["thumbnail"]:
                st.image(doc.metadata["thumbnail"], width=200)

            if "id" in doc.metadata:
                st.caption(f"üÜî ID: {doc.metadata['id']}")
        # retriever = vectorstore.as_retriever(
        #     search_type="similarity_score_threshold",
        #     search_kwargs={
        #         "score_threshold": 0.5,
        #         "k": 3
        #     }
        # )

        # docs = retriever.get_relevant_documents(question)

        # if docs:
        #     # 1. ‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏∏‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Å‡πà‡∏≠‡∏ô
        #     st.subheader("üìö ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡πÄ‡∏à‡∏≠:")
        #     for i, doc in enumerate(docs, start=1):
        #         st.markdown(f"### ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà {i}")
        #         st.write(doc.page_content)

        #         if doc.metadata.get("thumbnail"):
        #             st.image(doc.metadata["thumbnail"], width=200)

        #         st.caption(f"üÜî ID: {doc.metadata.get('id', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏')}")

        #     # 2. ‡∏™‡πà‡∏á context ‡πÄ‡∏Ç‡πâ‡∏≤ LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡∏∏‡∏õ
        #     context = "\n\n".join([doc.page_content for doc in docs])

        #     chain = custom_prompt | llm
        #     result = chain.invoke({
        #         "context": context,
        #         "question": question
        #     })

        #     st.divider()
        #     st.subheader("üìù ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏£‡∏∏‡∏õ:")
        #     st.write(result.content)

        # else:
        #     st.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö üïµÔ∏è‚Äç‚ôÇÔ∏è")




##
##‡∏Ç‡∏≠ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏≤‡∏¢‡∏î‡∏≠‡∏Å‡∏ö‡∏±‡∏ß‡∏ö‡∏≤‡∏ô